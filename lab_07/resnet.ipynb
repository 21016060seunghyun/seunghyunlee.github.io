{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Networks (ResNet)\n",
    "\n",
    "이번 실습에서는 매우 깊은 CNN모델인 <b>Residual Networks (ResNet)</b>를 직접 구현해봅니다. \n",
    "\n",
    "일반적으로 신경망이 깊어질수록(deep) 표현력(representational power)이 증가하여 복잡한 함수도 근사할할 수 있게 되지만, 학습이 어려워진다는 문제가 발생합니다.  \n",
    "\n",
    "논문 [He et al. (2015)](https://arxiv.org/pdf/1512.03385.pdf)에서 소개한 ResNet구조는 아주 깊은 신경망도 효과적으로 학습할 수 있는 방법을 제안하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 깊은 신경망의 문제점 (The Problem of Very Deep Neural Networks)\n",
    "\n",
    "- 깊은 신경망의 가장 중요한 장점은 아주 복잡한 함수도 근사할 수 있다는 것입니다. 입력에 가까운 앞쪽 레이어에서는 edge, texture와 같은 저수준 특징(low-level features)을 추출하고, 깊은 레이어로 갈수록 좀더 추상적이고 고수준의 특징(high-level features)을 계층적으로 추출함으로써 깊은 신경망은 아주 높은 표현력(representational power)을 가질 수 있습니다.\n",
    "- 하지만 무작정 깊게 쌓는다고 좋은 성능이 나오는 것은 아니며, 대표적인 원인은 **기울기 소실(vanishing gradient)** 문제입니다.\n",
    "\n",
    "### 기울기 소실 (vanishing gradient)\n",
    "- 역전파(backpropagation) 과정에서, 파라미터에 대한 손실 함수 $\\mathcal{L}$의 미분은 연쇄법칙(chain rule)에 의해 마지막 레이어부터 첫번째 레이어로 전파됩니다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{[1]}} =\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}^{[L]}} \\cdot\n",
    "\\left(\n",
    "\\prod_{l=L}^{2} \n",
    "\\frac{\\partial \\mathbf{a}^{[l]}}{\\partial \\mathbf{z}^{[l]}} \\cdot\n",
    "\\frac{\\partial \\mathbf{z}^{[l]}}{\\partial \\mathbf{a}^{[l-1]}} \n",
    "\\right) \\cdot\n",
    "\\frac{\\partial \\mathbf{a}^{[1]}}{\\partial \\mathbf{z}^{[1]}} \\cdot\n",
    "\\frac{\\partial \\mathbf{z}^{[1]}}{\\partial \\mathbf{W}^{[1]}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^{[l]} = \\mathbf{W}^{[l]} \\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}\n",
    "\\quad , \\quad\n",
    "\\mathbf{a}^{[l]} = \\phi(\\mathbf{z}^{[l]})\n",
    "$$\n",
    "\n",
    "\n",
    "- 이때 각 레이어에서의 편미분 값들이 $< 1$인 경우, 여러 레이어를 거치며 곱해지는 과정에서 gradient가 지수적으로 감소하며며 아주 빠르게 0에 가까워집니다.\n",
    "- 이는 곧 앞쪽 레이어(입력에 가까운 레이어)의 가중치가 거의 업데이트되지 않아, 학습 제대로 이루어지지 않음을 의미합니다.\n",
    "- (반대로 편미분 값들이 $> 1$인 경우에는 그래디언트가 레이어를 지날수록 지수적으로 증가하여 gradient explode현상이 발생할 수 있습니다. 일반적으로는 gradient vanishing현상이 더 자주 발생합니다.)\n",
    "\n",
    "<center><img src=\"resources/vanishing_grad_kiank.png\" style=\"width:450px;height:220px;\"></center>\n",
    "\n",
    "- 위 그림을 살펴보면, 입력에 가까운 레이어일수록 gradient의 크기(norm) 급격히 감소하여 학습이 이루어지기 어려움을 알 수 있습니다.\n",
    "\n",
    "## Core idea of ResNet\n",
    "\n",
    "ResNet에서는 <b>skip connection</b>(residual connection 또는 shortcut 이라고도 불림)이라고 불리는 구조를 도입하여 이 문제를 해결합니다.\n",
    "\n",
    "<center><img src=\"resources/skip_connection_kiank.png\" style=\"width:650px;height:200px;\"></center>\n",
    "\n",
    " - 왼쪽 그림은 일반적인 신경망의 연산 경로(main path)를 나타냅니다.  \n",
    " - 오른쪽 그림은 여기에 skip connection이 추가된 구조로, 입력이 일부 레이어를 건너뛰어 출력에 직접 더해집니다. ResNet에서는 이러한 구조를 <b>residual block</b>이라고 부릅니다\n",
    "\n",
    "### Skip connection의 장점\n",
    "- Skip connection은 역전파 시 기울기(gradient)가 직접 전달될 수 있는 경로를 제공하여, 기울기 소실(vanishing gradient) 현상을 크게 줄여줍니다.\n",
    "- 또한, Skip connection이 존재하는 <b>residual block</b>은 항등 함수(identity function)을 매우 쉽게 학습할 수 있습니다.\n",
    "  - 예를 들어 블록 내 모든 가중치가 0이면, 출력은 입력과 거의 같아져 항등 함수가 됩니다.\n",
    "  - 이는 residual block을 아주 깊게 쌓더라도, 더 얕은 네트워크에 비해 성능이 나빠지지 않는다는 특성을 갖게 합니다.\n",
    "  - 실제로 일부 연구에서는, ResNet의 성능 향상이 vanishing gradient를 막는 것보다 항등 함수를 빠르게 학습할 수 있는 구조에서 기인한다는 분석도 제시하고 있습니다.\n",
    "\n",
    "따라서 ResNet은 이러한 **residual block**을 반복적으로 쌓는 방식으로, 수십~수백 층에 이르는 매우 깊은 신경망도 안정적으로 학습할 수 있게 해줍니다.\n",
    "\n",
    "## Building a ResNet from scratch\n",
    "\n",
    "ResNet에서는 입력과 출력의 차원에 따라 두 가지 종류의 블록을 사용합니다:\n",
    "- Identity Block: 입력과 출력의 차원이 같은 경우  \n",
    "- Convolutional Block: 차원이 다른 경우\n",
    "\n",
    "### Identity Block\n",
    "\n",
    "identity block은 ResNet의 기본 블럭으로, 입력 activation(예 $a^{[l]}$)과 출력 activation (예 $a^{[l+2]}$)의 **차원이 같을 때** 사용됩니다.\n",
    "\n",
    "### Basic Identity block (ResNet-18, ResNet-34)\n",
    "<center><img src=\"resources/idblock2_kiank.png\" style=\"width:650px;height:150px;\"></center>\n",
    "<caption><center> <b>2개의 레이어를 건너뛰는 Basic Identity block.</b> </center></caption>\n",
    "\n",
    "- 위 구조는는 2개의 conv 레이어와 skip connection으로 구성됩니다.\n",
    "- 이러한 구조는 ResNet-18, ResNet-34에서 사용됩니다.\n",
    "\n",
    "### Bottleneck Identity block (ResNet-50 이후)\n",
    "ResNet-50부터는 더 깊고 네트워크를 효율적으로 학습하기 위해 Bottleneck 구조의 identity block을 사용합니다.\n",
    "\n",
    "<center><img src=\"resources/idblock3_kiank.png\" style=\"width:650px;height:150px;\"></center>\n",
    "    <caption><center> <b>3개의 레이어를 건너뛰는 Bottleneck Identity block.</b> (Resnet50 이후) </center></caption>\n",
    "\n",
    "이 블록은 다음과 같은 단계로 구성됩니다:\n",
    "1. 1x1 conv: 채널 수를 감소시킴.\n",
    "2. 3x3 conv: 특징 추출\n",
    "3. 1x1 conv: 채널 수를 다시 복원\n",
    "4. Skip connection: 입력을 출력에 더해줌.\n",
    "\n",
    "이처럼 채널 수를 먼저 줄인 뒤 연산을 수행하고, 다시 채널 수를 원래대로 늘려주는 구조를 Bottleneck 구조라고 부릅니다.\n",
    "\n",
    "<center><img src=\"resources/Basic Block_1.png\" style=\"width:264px;\">  <img src=\"resources/Bottleneck Block_2.png\" style=\"width:300px;\"></center>\n",
    "\n",
    "- bottleneck 구조는 레이어 수는 더 많지만, 채널 수를 줄인 후 3x3 conv를 수행하기 때문에 연산량인 더 적고 효율적입니다.\n",
    "- 또한 비선형성(non-linearity)이 더 많아 표현력이 향상되며, 더 적은 연산량으로도 비슷하거나 더 좋은 성능을 달성합니다.\n",
    "\n",
    "<mark>실습</mark> Bottleneck Identity block을 구현하세요\n",
    "\n",
    "1. main path의 첫번째 layer\n",
    "   - `Conv2d`: `intermediate_channels`개의 1x1 필터와 stride = 1, no zero padding (\"valid\" padding). BatchNorm을 수행할 것이므로 `bias = False`이다.\n",
    "   - `BatchNorm2d`: 'channels'축을 기준으로 정규화.\n",
    "   - `ReLU`\n",
    "\n",
    "2. main path의 두번쨰 layer\n",
    "   - `Conv2d`: `intermediate_channels`개의 3x3 필터와 stride = 1, \"same\" padding (출력 feature map의 크기가 입력과 동일), `bias = False`\n",
    "   - `BatchNorm2d`\n",
    "   - `ReLU`\n",
    "\n",
    "3. main path의 세번째 layer\n",
    "   - `Conv2d`: `intermediate_channels` x `expansion`개의 1x1 필터와 stride = 1, no zero padding, `bias = False`\n",
    "   - `BatchNorm2d`\n",
    "   - <b>NO</b> `ReLU` activation\n",
    "\n",
    "4. Skip connection (Shortcut path)\n",
    "   - main path의 결과와 입력값(`x`)을 더해준다 (element-wise addition)\n",
    "   - 그 후 `ReLU` activation을 적용한다.\n",
    "\n",
    "아래 documentation을 참고하세요\n",
    "- [Conv2D](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
    "- [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)\n",
    "- [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityBlock(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, intermediate_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        ##### YOUR CODE START #####\n",
    "        self.conv1 = nn.Conv2d(in_channels, intermediate_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(intermediate_channels)\n",
    "        self.conv2 = nn.Conv2d(intermediate_channels, intermediate_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(intermediate_channels)\n",
    "        self.conv3 = nn.Conv2d(intermediate_channels, intermediate_channels * self.expansion, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn3   = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "       ##### YOUR CODE END #####\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        ##### YOUR CODE END #####\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape:  torch.Size([4, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass\n",
    "identity_block = IdentityBlock(in_channels=64, intermediate_channels=16)\n",
    "\n",
    "X = torch.rand(4, 64, 32, 32) # dummy data for testing with batch_size 4\n",
    "out = identity_block(X) \n",
    "\n",
    "print(\"out.shape: \", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Block\n",
    "\n",
    "\"convolutional block\"은 ResNet에서 사용하는 두 번째 유형의 블록으로, <b>입력과 출력의 차원이 다를때</b> 사용하는 구조입니다.\n",
    "\n",
    "indentity block과의 주요 차이점은 shortcut path에 `Conv2D` 레이어가 포함되어 있다는 점입니다.\n",
    "\n",
    "<center><img src=\"resources/convblock_kiank.png\" style=\"width:650px;height:150px;\"></center>\n",
    "<caption><center> <b>Convolutional block</b> </center></caption>\n",
    "\n",
    "* shortcut path의 `Conv2D`레이어는 입력 $x$의 차원을 main path의 출력 차원과 일치하도록록 변환하는 역할을 합니다다.\n",
    "  * 예를들어 입력의 높이(height)와 너비(width)를 1/2로 줄이고 싶다면 1x1 conv를 `stride = 2`로 적용합니다. \n",
    "  * 이 `Conv2D` 레이어의 출력값에는 비선형 활성화 함수(non-linear activation)를 적용하지 않으며, 입력에 대한 단순한 선형 변환을 수행하는 것이 목적입니다.\n",
    "\n",
    "<mark>실습</mark> Convolution Block을 구현하세요\n",
    "\n",
    "1. main path의 첫번째 레이어\n",
    "   - `Conv2d`: `intermediate_channels`개의 1x1 필터와 stride = 1, no zero padding, `bias = False`\n",
    "   - `BatchNorm2d`\n",
    "   - `ReLU` activation\n",
    "\n",
    "2. main path의 두번째 레이어\n",
    "   - `Conv2d`: `intermediate_channels`개의 3x3필터와 stride = stride, padding = 1, `bias = False`\n",
    "   - `BatchNorm2d`\n",
    "   - `ReLU` activation\n",
    "\n",
    "3. main path의 세번째 레이어\n",
    "   - `Conv2d`: `intermediate_channels` x `expansion`개의 1x1 필터와 stride = 1, no zero padding, `bias = False`\n",
    "   - `BatchNorm2d`\n",
    "   - **NO** ReLU activation \n",
    "\n",
    "4. Shortcut path\n",
    "   - `Conv2d`: `intermediate_channels` x `expansion`개의 1x1 필터와 stride = stride, no zero padding, `bias = False`\n",
    "   - `BatchNorm2d`\n",
    "\n",
    "5. Final step: \n",
    "   - main path의 출력과 shortcut path의 출력을 더합니다.\n",
    "   - 그 후 `ReLU` activation을 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, intermediate_channels, stride):\n",
    "        super().__init__()\n",
    "\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, intermediate_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(intermediate_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(intermediate_channels, intermediate_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(intermediate_channels)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(intermediate_channels, intermediate_channels * self.expansion, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn3   = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
    "        \n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, intermediate_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##### YOUR CODE START #####\n",
    "        \n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        identity = self.shortcut(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape:  torch.Size([4, 64, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass\n",
    "conv_block = ConvBlock(in_channels=64, intermediate_channels=16, stride=2)\n",
    "\n",
    "X = torch.rand(4, 64, 32, 32) # dummy data for testing with batch_size 4\n",
    "out = conv_block(X) \n",
    "\n",
    "print(\"out.shape: \", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the ResNet-50 Architecture\n",
    "\n",
    "<center><img src=\"resources/resnet_kiank.png\" style=\"width:850px;height:150px;\"></center>\n",
    "<caption><center> <b>ResNet-50 architecture</b> </center></caption>\n",
    "- 위 그림은 ResNet-50의 전체 구조를 보여줍니다.\n",
    "- 여기서 \"ID BLOCK\"은 Identity block을, \"ID BLOCK x3\"은 identity block을 3번번 쌓는것을 의미합니다.\n",
    "\n",
    "\n",
    "<mark>실습</mark> 50개의 레이어가 있는 ResNet-50 아키텍쳐를 구현하세요.\n",
    "- Stage 1 (Stem):\n",
    "    - `Conv2D': 64개의 7x7 필터, stride = 2, padding = 3, bias=False, 입력은 RGB 이미지입니다.\n",
    "    - `BatchNorm2d`\n",
    "    - `ReLU` activation\n",
    "    - `MaxPool2d`: stride = 2, kernel_size = 3, padding = 1\n",
    "- Stage 2 (3 blocks):\n",
    "    - `ConvBlock`: intermediate_channels = 64, expansion = 4, stride = 1\n",
    "    - `IdentityBlock` x 2:  intermediate_channels = 64, expansion = 4\n",
    "- Stage 3 (4 blocks):\n",
    "    - `ConvBlock`: intermediate_channels = 128, expansion = 4, stride = 2\n",
    "    - `IdentityBlock` x 3: intermediate_channels = 128, expansion = 4\n",
    "- Stage 4 (6 blocks):\n",
    "    - `ConvBlock`: intermediate_channels = 256, expansion = 4, stride = 2\n",
    "    -`IdentityBlock` x 5:  intermediate_channels = 256, expansion = 4\n",
    "- Stage 5 (3 blocks):\n",
    "    - `ConvBlock`: intermediate_channels = 512, expansion = 4, stride = 2\n",
    "    - `IdentityBlock` x 2: intermediate_channels = 512, expansion = 4\n",
    "- `AdaptiveAvgPool2d`: 2D Average Pooling (output shape = Cx1x1)\n",
    "- `Flatten` layer\n",
    "- `Linear`: out_features = `num_classes`\n",
    "\n",
    "아래 문서를 참고하세요\n",
    "- [MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)\n",
    "- [flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html)\n",
    "- [AdaptiveAvgPool2d](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet50, self).__init__()\n",
    "\n",
    "                ##### YOUR CODE START #####\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ConvBlock(64, 64, stride=1),\n",
    "            IdentityBlock(256, 64),\n",
    "            IdentityBlock(256, 64)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ConvBlock(256, 128, stride=2),\n",
    "            IdentityBlock(512, 128),\n",
    "            IdentityBlock(512, 128),\n",
    "            IdentityBlock(512, 128)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ConvBlock(512, 256, stride=2),\n",
    "            IdentityBlock(1024, 256),\n",
    "            IdentityBlock(1024, 256),\n",
    "            IdentityBlock(1024, 256),\n",
    "            IdentityBlock(1024, 256),\n",
    "            IdentityBlock(1024, 256)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            ConvBlock(1024, 512, stride=2),\n",
    "            IdentityBlock(2048, 512),\n",
    "            IdentityBlock(2048, 512)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "             ##### YOUR CODE END #####\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##### YOUR CODE START #####\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.fc(x)\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape:  torch.Size([4, 1000])\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass\n",
    "model = ResNet50(num_classes = 1000)\n",
    "\n",
    "X = torch.rand(4, 3, 224, 224) # dummy data for testing with batch_size 4\n",
    "logits = model(X) \n",
    "\n",
    "print(\"logits.shape: \", logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직접 구현한 ResNet-50과 PyTorch에 미리 구현되어있는 ResNet-50의 파라미터 수가 같음을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of parameters of your ResNet-50 model \t: 25557032\n",
      "# of parameters of pre-implemented model \t: 25557032\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "torch_model = models.resnet50()\n",
    "print(f\"# of parameters of your ResNet-50 model \\t: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"# of parameters of pre-implemented model \\t: {sum(p.numel() for p in torch_model.parameters())}\")\n",
    "\n",
    "assert sum(p.numel() for p in model.parameters()) == sum(p.numel() for p in torch_model.parameters()), \"The number of parameters of your ResNet-50 model and the pre-implemented model are not same. Please check your implementation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>실습</mark> 앞서 정의한 `model = ResNet50(num_classes = 1000)`모델에 입력 이미지 `X = torch.rand(4, 3, 224, 224)`를 통과시켰을때, 각 stage를 지난 후 출력 텐서의 shape을 조사하여 아래 변수들에 채워 넣으세요.\n",
    "- 각 shape은 튜플(tuple) 형태로 작성하세요.\n",
    "- 튜플의 각 원서(element)는 숫자 계산식으로 입력해도 괜찮으나, 파이썬 변수를 사용하지 마세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (4, 3, 224, 224)\n",
    "shape_after_stage1 = (4, 64, 56, 56)  # TODO: output shape after the stage1 (stem)\n",
    "shape_after_stage2 = (4, 256, 56, 56)   # TODO: output shape after the stage2\n",
    "shape_after_stage3 = (4, 512, 28, 28) # TODO: output shape after the stage3\n",
    "shape_after_stage4 = (4, 1024, 14, 14) # TODO: output shape after the stage4\n",
    "shape_after_stage5 = (4, 2048, 7, 7)  # TODO: output shape after the stage5\n",
    "shape_after_avgpool = (4, 2048, 1, 1)  # TODO: output shape after the avgpool\n",
    "shape_after_flatten = (4, 2048)   # TODO: output shape after the flatten\n",
    "shape_after_fc = (4, 1000)      # TODO: output shape after the fc layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from train_utils import train_model, load_and_evaluate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전이 학습(Transfer Learning)은 대규모 데이터셋을 이용해 <b>사전 학습(pretraining)</b>된 모델을 기반으로,\n",
    "보다 작은 규모의 타겟 데이터셋(target dataset)에 대해 파인튜닝(fine-tuning)을 수행함으로써, 적은 양의 데이터로도 높은 성능을 낼 수 있는 학습 방법입니다.\n",
    "\n",
    "이번 실습 시간에는 ImageNet 데이터셋(약 120만장의 이미지, 1,000 classes, ~140 GB)으로 사전학습된 ResNet-50모델을 이용하여 CIFAR-10 데이터셋을 학습해봅니다.\n",
    "\n",
    "이를 위해서는 먼저 이미지의 전처리(`transform`) 방식을 사전 학습된 모델과 일치하도록 맞추어 주어야 합니다.\n",
    " - <b>정규화(standardization)</b>: ImageNet 데이터 학습시 사용한 평균/표준편차 값을 그대로 사용해야 합니다 : `mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]`\n",
    " - <b>입력 이미지 크기 조정</b>: 사전학습된 모델은 `(224, 224)`크기의 이미지를 입력으로 받도록 설계되어 있으므로, CIFAR-10 데이터셋도 이 크기로 `Resize`합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_datasets(data_root_dir):\n",
    "    normalize = transforms.Normalize(mean = [0.485, 0.456, 0.406],\n",
    "                                     std = [0.229, 0.224, 0.225])  # pretraining에서 사용한 normalize로 수정\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)), # pretraining에서 사용된 이미지 사이즈로 수정\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    eval_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)), # pretraining에서 사용된 이미지 사이즈로 수정\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    train_dataset_full = datasets.CIFAR10(root=data_root_dir, train=True, download=False, transform=train_transforms)\n",
    "    val_dataset_full = datasets.CIFAR10(root=data_root_dir, train=True, download=False, transform=eval_transforms)\n",
    "    test_dataset = datasets.CIFAR10(root=data_root_dir, train=False, download=False, transform=eval_transforms)\n",
    "\n",
    "    train_indices, val_indices = train_test_split(np.arange(len(train_dataset_full)), \n",
    "                                                  test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    train_dataset = Subset(train_dataset_full, train_indices)\n",
    "    val_dataset = Subset(val_dataset_full, val_indices)\n",
    "\n",
    "    train_dataset.classes = train_dataset_full.classes  # monkey‐patch class informations\n",
    "    train_dataset.class_to_idx = train_dataset_full.class_to_idx\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전학습된 ResNet-50 모델 불러오기\n",
    "\n",
    "PyTorch에서는 [링크](https://pytorch.org/vision/stable/models.html#table-of-all-available-classification-weights)와 같이 다양한 사전학습 모델(pre-trained model)들을 제공합니다.\n",
    "\n",
    "아래 코드와 같이 `torchvision.models` 모듈을 통해 ImageNet 데이터셋으로 사전학습된 ResNet-50 모델을 불러올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "model = models.resnet50(weights = 'IMAGENET1K_V2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 ResNet-50 모델의 전체 구조를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet-50은 여러 개의 서브 모듈(sub-module) 로 구성되어 있으며, `.` 연산자를 사용해 각 구성 요소에 접근할 수 있습니다.\n",
    "\n",
    "예를 들어, `model.layer1`과 같이 입력하면 서브 모듈 `layer1`에 접근할 수 있습니다.\n",
    " - `layer1` ~ `layer4`는 우리가 앞서 앞서 직접 구현했던 ResNet-50 아키텍처의 Stage 2 ~ Stage 5에 해당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1:  Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"layer1: \", model.layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-turning을 위해 모델 수정하기\n",
    "사전 학습된 ResNet-50 모델을 이용하여 CIFAR-10 데이터을 학습하기 위해서는, 다음과 같이 모델 구조를 일부 수정해야 합니다:\n",
    "1. 마지막 `Linear` 레이어의 출력 차원(`out_features`)을 CIFAR-10 데이터셋의 클래스 수(num_classes = 10)와 일치하도록 수정\n",
    "2. 학습 대상 파라미터 설정: 필요한 레이어만 학습하도록, 특정 레이어는 동결(freeze) 하고, 나머지는 학습 가능(trainable) 하도록 설정합니다.\n",
    "\n",
    "### 마지막 fc 레이어 수정하기\n",
    "\n",
    "사전 학습된 ResNet-50 모델의 마지막 fully connected 레이어(`fc`)를 다음과 같이 새로 정의할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight                   Param shape: torch.Size([64, 3, 7, 7])      requires_grad: False\n",
      "bn1.weight                     Param shape: torch.Size([64])               requires_grad: False\n",
      "bn1.bias                       Param shape: torch.Size([64])               requires_grad: False\n",
      "layer1.0.conv1.weight          Param shape: torch.Size([64, 64, 1, 1])     requires_grad: False\n",
      "layer1.0.bn1.weight            Param shape: torch.Size([64])               requires_grad: False\n",
      "layer1.0.bn1.bias              Param shape: torch.Size([64])               requires_grad: False\n",
      "layer1.0.conv2.weight          Param shape: torch.Size([64, 64, 3, 3])     requires_grad: False\n",
      "layer1.0.bn2.weight            Param shape: torch.Size([64])               requires_grad: False\n",
      "layer1.0.bn2.bias              Param shape: torch.Size([64])               requires_grad: False\n",
      "layer1.0.conv3.weight          Param shape: torch.Size([256, 64, 1, 1])    requires_grad: False\n",
      "layer1.0.bn3.weight            Param shape: torch.Size([256])              requires_grad: False\n",
      "layer1.0.bn3.bias              Param shape: torch.Size([256])              requires_grad: False\n",
      "layer1.0.downsample.0.weight   Param shape: torch.Size([256, 64, 1, 1])    requires_grad: False\n",
      "layer1.0.downsample.1.weight   Param shape: torch.Size([256])              requires_grad: False\n",
      "layer1.0.downsample.1.bias     Param shape: torch.Size([256])              requires_grad: False\n",
      "layer1.1.conv1.weight          Param shape: torch.Size([64, 256, 1, 1])    requires_grad: False\n",
      "layer1.1.bn1.weight            Param shape: torch.Size([64])               requires_grad: False\n",
      "layer1.1.bn1.bias              Param shape: torch.Size([64])               requires_grad: False\n",
      "layer1.1.conv2.weight          Param shape: torch.Size([64, 64, 3, 3])     requires_grad: False\n",
      "layer1.1.bn2.weight            Param shape: torch.Size([64])               requires_grad: False\n",
      "layer1.1.bn2.bias              Param shape: torch.Size([64])               requires_grad: False\n",
      "layer1.1.conv3.weight          Param shape: torch.Size([256, 64, 1, 1])    requires_grad: False\n",
      "layer1.1.bn3.weight            Param shape: torch.Size([256])              requires_grad: False\n",
      "layer1.1.bn3.bias              Param shape: torch.Size([256])              requires_grad: False\n",
      "layer1.2.conv1.weight          Param shape: torch.Size([64, 256, 1, 1])    requires_grad: False\n",
      "layer1.2.bn1.weight            Param shape: torch.Size([64])               requires_grad: False\n",
      "layer1.2.bn1.bias              Param shape: torch.Size([64])               requires_grad: False\n",
      "layer1.2.conv2.weight          Param shape: torch.Size([64, 64, 3, 3])     requires_grad: False\n",
      "layer1.2.bn2.weight            Param shape: torch.Size([64])               requires_grad: False\n",
      "layer1.2.bn2.bias              Param shape: torch.Size([64])               requires_grad: False\n",
      "layer1.2.conv3.weight          Param shape: torch.Size([256, 64, 1, 1])    requires_grad: False\n",
      "layer1.2.bn3.weight            Param shape: torch.Size([256])              requires_grad: False\n",
      "layer1.2.bn3.bias              Param shape: torch.Size([256])              requires_grad: False\n",
      "layer2.0.conv1.weight          Param shape: torch.Size([128, 256, 1, 1])   requires_grad: False\n",
      "layer2.0.bn1.weight            Param shape: torch.Size([128])              requires_grad: False\n",
      "layer2.0.bn1.bias              Param shape: torch.Size([128])              requires_grad: False\n",
      "layer2.0.conv2.weight          Param shape: torch.Size([128, 128, 3, 3])   requires_grad: False\n",
      "layer2.0.bn2.weight            Param shape: torch.Size([128])              requires_grad: False\n",
      "layer2.0.bn2.bias              Param shape: torch.Size([128])              requires_grad: False\n",
      "layer2.0.conv3.weight          Param shape: torch.Size([512, 128, 1, 1])   requires_grad: False\n",
      "layer2.0.bn3.weight            Param shape: torch.Size([512])              requires_grad: False\n",
      "layer2.0.bn3.bias              Param shape: torch.Size([512])              requires_grad: False\n",
      "layer2.0.downsample.0.weight   Param shape: torch.Size([512, 256, 1, 1])   requires_grad: False\n",
      "layer2.0.downsample.1.weight   Param shape: torch.Size([512])              requires_grad: False\n",
      "layer2.0.downsample.1.bias     Param shape: torch.Size([512])              requires_grad: False\n",
      "layer2.1.conv1.weight          Param shape: torch.Size([128, 512, 1, 1])   requires_grad: False\n",
      "layer2.1.bn1.weight            Param shape: torch.Size([128])              requires_grad: False\n",
      "layer2.1.bn1.bias              Param shape: torch.Size([128])              requires_grad: False\n",
      "layer2.1.conv2.weight          Param shape: torch.Size([128, 128, 3, 3])   requires_grad: False\n",
      "layer2.1.bn2.weight            Param shape: torch.Size([128])              requires_grad: False\n",
      "layer2.1.bn2.bias              Param shape: torch.Size([128])              requires_grad: False\n",
      "layer2.1.conv3.weight          Param shape: torch.Size([512, 128, 1, 1])   requires_grad: False\n",
      "layer2.1.bn3.weight            Param shape: torch.Size([512])              requires_grad: False\n",
      "layer2.1.bn3.bias              Param shape: torch.Size([512])              requires_grad: False\n",
      "layer2.2.conv1.weight          Param shape: torch.Size([128, 512, 1, 1])   requires_grad: False\n",
      "layer2.2.bn1.weight            Param shape: torch.Size([128])              requires_grad: False\n",
      "layer2.2.bn1.bias              Param shape: torch.Size([128])              requires_grad: False\n",
      "layer2.2.conv2.weight          Param shape: torch.Size([128, 128, 3, 3])   requires_grad: False\n",
      "layer2.2.bn2.weight            Param shape: torch.Size([128])              requires_grad: False\n",
      "layer2.2.bn2.bias              Param shape: torch.Size([128])              requires_grad: False\n",
      "layer2.2.conv3.weight          Param shape: torch.Size([512, 128, 1, 1])   requires_grad: False\n",
      "layer2.2.bn3.weight            Param shape: torch.Size([512])              requires_grad: False\n",
      "layer2.2.bn3.bias              Param shape: torch.Size([512])              requires_grad: False\n",
      "layer2.3.conv1.weight          Param shape: torch.Size([128, 512, 1, 1])   requires_grad: False\n",
      "layer2.3.bn1.weight            Param shape: torch.Size([128])              requires_grad: False\n",
      "layer2.3.bn1.bias              Param shape: torch.Size([128])              requires_grad: False\n",
      "layer2.3.conv2.weight          Param shape: torch.Size([128, 128, 3, 3])   requires_grad: False\n",
      "layer2.3.bn2.weight            Param shape: torch.Size([128])              requires_grad: False\n",
      "layer2.3.bn2.bias              Param shape: torch.Size([128])              requires_grad: False\n",
      "layer2.3.conv3.weight          Param shape: torch.Size([512, 128, 1, 1])   requires_grad: False\n",
      "layer2.3.bn3.weight            Param shape: torch.Size([512])              requires_grad: False\n",
      "layer2.3.bn3.bias              Param shape: torch.Size([512])              requires_grad: False\n",
      "layer3.0.conv1.weight          Param shape: torch.Size([256, 512, 1, 1])   requires_grad: False\n",
      "layer3.0.bn1.weight            Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.0.bn1.bias              Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.0.conv2.weight          Param shape: torch.Size([256, 256, 3, 3])   requires_grad: False\n",
      "layer3.0.bn2.weight            Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.0.bn2.bias              Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.0.conv3.weight          Param shape: torch.Size([1024, 256, 1, 1])  requires_grad: False\n",
      "layer3.0.bn3.weight            Param shape: torch.Size([1024])             requires_grad: False\n",
      "layer3.0.bn3.bias              Param shape: torch.Size([1024])             requires_grad: False\n",
      "layer3.0.downsample.0.weight   Param shape: torch.Size([1024, 512, 1, 1])  requires_grad: False\n",
      "layer3.0.downsample.1.weight   Param shape: torch.Size([1024])             requires_grad: False\n",
      "layer3.0.downsample.1.bias     Param shape: torch.Size([1024])             requires_grad: False\n",
      "layer3.1.conv1.weight          Param shape: torch.Size([256, 1024, 1, 1])  requires_grad: False\n",
      "layer3.1.bn1.weight            Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.1.bn1.bias              Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.1.conv2.weight          Param shape: torch.Size([256, 256, 3, 3])   requires_grad: False\n",
      "layer3.1.bn2.weight            Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.1.bn2.bias              Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.1.conv3.weight          Param shape: torch.Size([1024, 256, 1, 1])  requires_grad: False\n",
      "layer3.1.bn3.weight            Param shape: torch.Size([1024])             requires_grad: False\n",
      "layer3.1.bn3.bias              Param shape: torch.Size([1024])             requires_grad: False\n",
      "layer3.2.conv1.weight          Param shape: torch.Size([256, 1024, 1, 1])  requires_grad: False\n",
      "layer3.2.bn1.weight            Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.2.bn1.bias              Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.2.conv2.weight          Param shape: torch.Size([256, 256, 3, 3])   requires_grad: False\n",
      "layer3.2.bn2.weight            Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.2.bn2.bias              Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.2.conv3.weight          Param shape: torch.Size([1024, 256, 1, 1])  requires_grad: False\n",
      "layer3.2.bn3.weight            Param shape: torch.Size([1024])             requires_grad: False\n",
      "layer3.2.bn3.bias              Param shape: torch.Size([1024])             requires_grad: False\n",
      "layer3.3.conv1.weight          Param shape: torch.Size([256, 1024, 1, 1])  requires_grad: False\n",
      "layer3.3.bn1.weight            Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.3.bn1.bias              Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.3.conv2.weight          Param shape: torch.Size([256, 256, 3, 3])   requires_grad: False\n",
      "layer3.3.bn2.weight            Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.3.bn2.bias              Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.3.conv3.weight          Param shape: torch.Size([1024, 256, 1, 1])  requires_grad: False\n",
      "layer3.3.bn3.weight            Param shape: torch.Size([1024])             requires_grad: False\n",
      "layer3.3.bn3.bias              Param shape: torch.Size([1024])             requires_grad: False\n",
      "layer3.4.conv1.weight          Param shape: torch.Size([256, 1024, 1, 1])  requires_grad: False\n",
      "layer3.4.bn1.weight            Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.4.bn1.bias              Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.4.conv2.weight          Param shape: torch.Size([256, 256, 3, 3])   requires_grad: False\n",
      "layer3.4.bn2.weight            Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.4.bn2.bias              Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.4.conv3.weight          Param shape: torch.Size([1024, 256, 1, 1])  requires_grad: False\n",
      "layer3.4.bn3.weight            Param shape: torch.Size([1024])             requires_grad: False\n",
      "layer3.4.bn3.bias              Param shape: torch.Size([1024])             requires_grad: False\n",
      "layer3.5.conv1.weight          Param shape: torch.Size([256, 1024, 1, 1])  requires_grad: False\n",
      "layer3.5.bn1.weight            Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.5.bn1.bias              Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.5.conv2.weight          Param shape: torch.Size([256, 256, 3, 3])   requires_grad: False\n",
      "layer3.5.bn2.weight            Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.5.bn2.bias              Param shape: torch.Size([256])              requires_grad: False\n",
      "layer3.5.conv3.weight          Param shape: torch.Size([1024, 256, 1, 1])  requires_grad: False\n",
      "layer3.5.bn3.weight            Param shape: torch.Size([1024])             requires_grad: False\n",
      "layer3.5.bn3.bias              Param shape: torch.Size([1024])             requires_grad: False\n",
      "layer4.0.conv1.weight          Param shape: torch.Size([512, 1024, 1, 1])  requires_grad: False\n",
      "layer4.0.bn1.weight            Param shape: torch.Size([512])              requires_grad: False\n",
      "layer4.0.bn1.bias              Param shape: torch.Size([512])              requires_grad: False\n",
      "layer4.0.conv2.weight          Param shape: torch.Size([512, 512, 3, 3])   requires_grad: False\n",
      "layer4.0.bn2.weight            Param shape: torch.Size([512])              requires_grad: False\n",
      "layer4.0.bn2.bias              Param shape: torch.Size([512])              requires_grad: False\n",
      "layer4.0.conv3.weight          Param shape: torch.Size([2048, 512, 1, 1])  requires_grad: False\n",
      "layer4.0.bn3.weight            Param shape: torch.Size([2048])             requires_grad: False\n",
      "layer4.0.bn3.bias              Param shape: torch.Size([2048])             requires_grad: False\n",
      "layer4.0.downsample.0.weight   Param shape: torch.Size([2048, 1024, 1, 1]) requires_grad: False\n",
      "layer4.0.downsample.1.weight   Param shape: torch.Size([2048])             requires_grad: False\n",
      "layer4.0.downsample.1.bias     Param shape: torch.Size([2048])             requires_grad: False\n",
      "layer4.1.conv1.weight          Param shape: torch.Size([512, 2048, 1, 1])  requires_grad: False\n",
      "layer4.1.bn1.weight            Param shape: torch.Size([512])              requires_grad: False\n",
      "layer4.1.bn1.bias              Param shape: torch.Size([512])              requires_grad: False\n",
      "layer4.1.conv2.weight          Param shape: torch.Size([512, 512, 3, 3])   requires_grad: False\n",
      "layer4.1.bn2.weight            Param shape: torch.Size([512])              requires_grad: False\n",
      "layer4.1.bn2.bias              Param shape: torch.Size([512])              requires_grad: False\n",
      "layer4.1.conv3.weight          Param shape: torch.Size([2048, 512, 1, 1])  requires_grad: False\n",
      "layer4.1.bn3.weight            Param shape: torch.Size([2048])             requires_grad: False\n",
      "layer4.1.bn3.bias              Param shape: torch.Size([2048])             requires_grad: False\n",
      "layer4.2.conv1.weight          Param shape: torch.Size([512, 2048, 1, 1])  requires_grad: False\n",
      "layer4.2.bn1.weight            Param shape: torch.Size([512])              requires_grad: False\n",
      "layer4.2.bn1.bias              Param shape: torch.Size([512])              requires_grad: False\n",
      "layer4.2.conv2.weight          Param shape: torch.Size([512, 512, 3, 3])   requires_grad: False\n",
      "layer4.2.bn2.weight            Param shape: torch.Size([512])              requires_grad: False\n",
      "layer4.2.bn2.bias              Param shape: torch.Size([512])              requires_grad: False\n",
      "layer4.2.conv3.weight          Param shape: torch.Size([2048, 512, 1, 1])  requires_grad: False\n",
      "layer4.2.bn3.weight            Param shape: torch.Size([2048])             requires_grad: False\n",
      "layer4.2.bn3.bias              Param shape: torch.Size([2048])             requires_grad: False\n",
      "fc.weight                      Param shape: torch.Size([10, 2048])         requires_grad: True\n",
      "fc.bias                        Param shape: torch.Size([10])               requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# First, freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last layer\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:<30} Param shape: {str(param.shape):<30} requires_grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 대상 파라미터 설정\n",
    "\n",
    "모델 파라미터의 `require_grad`값을 조정함으로써, 원하는 레이어의 파라미터들을 동결(freeze) 하거나 학습 가능(trainable) 하도록 설정할 수 있습니다.\n",
    "\n",
    "아래 예시는 마지막 `fc` 레이어만 학습가능하도록(`requires_grad = True`) 설정하는 방법을 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>실습</mark> `get_model` 함수를 완성하세요.\n",
    "- `layer4`와 `fc`만 학습 가능하도록 설정하고, 나머지 파라미터들은 모두 동결(freeze)하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config, num_classes):\n",
    "    if config[\"model_architecture\"] == \"resnet50\":\n",
    "        print(f'Using pretrained model {config[\"pretrained\"]}')\n",
    "        model = models.resnet50(weights = config[\"pretrained\"])\n",
    "\n",
    "        ##### YOUR CODE START #####\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model architecture: {config['model_architecture']}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pretrained model IMAGENET1K_V2\n",
      "Total number of parameters: 23528522\n",
      "Number of trainable parameters: 14985226\n"
     ]
    }
   ],
   "source": [
    "model = get_model(config = {\"model_architecture\": \"resnet50\", \"pretrained\": \"IMAGENET1K_V2\"}, num_classes = 10)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(f\"Number of trainable parameters: {trainable_params}\")\n",
    "\n",
    "assert total_params == 23528522, f\"Total number of parameters is not correct.\"\n",
    "assert trainable_params == 14985226, f\"Number of trainable parameters is not correct.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>실습</mark> 아래 코드를 통해 사전 학습된 ResNet-50 모델을 사용해 CIFAR-10 데이터셋을 학습하고, 성능을 평가해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"mode\": \"train\",  # Options: \"train\", \"eval\"\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "\n",
    "    ## data and preprocessing settings\n",
    "    \"data_root_dir\": '/datasets',\n",
    "    \"num_workers\": 4,\n",
    "\n",
    "    ## model settings\n",
    "    \"model_architecture\": \"resnet50\",\n",
    "    'pretrained' : 'IMAGENET1K_V2',\n",
    "\n",
    "    ## Training Hyperparams\n",
    "    \"batch_size\": 64,       # use 1.7GB of GPU memory\n",
    "    \"learning_rate\": 1e-2,\n",
    "    \"num_epochs\": 10,\n",
    "\n",
    "    ## checkpoints\n",
    "    \"checkpoint_path\": \"checkpoints/checkpoint.pth\",    # Path to save the most recent checkpoint\n",
    "    \"best_model_path\": \"checkpoints/best_model.pth\",    # Path to save the best model checkpoint\n",
    "    \"checkpoint_save_interval\": 10,                     # Save a checkpoint every N epochs\n",
    "    \"resume_training\": None,    # Options: \"latest\", \"best\", or None\n",
    "\n",
    "    ## WandB logging\n",
    "    \"wandb_project_name\": \"CIFAR10-experiments\",\n",
    "    \"wandb_experiment_name\" : \"ResNet-50_pretrained\",\n",
    "    \"dataset_name\": \"CIFAR10\"\n",
    "}\n",
    "\n",
    "def main_train_resnet50(config):\n",
    "    train_dataset, val_dataset, test_dataset = load_cifar10_datasets(config[\"data_root_dir\"])\n",
    "    num_classes = len(train_dataset.classes)\n",
    "\n",
    "    model = get_model(config, num_classes = num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = config[\"learning_rate\"])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "    print(f\"Using {config['device']} device\")\n",
    "    print(f\"Number of model parameters: {sum(p.numel() for p in model.parameters())} ({sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable)\")\n",
    "    \n",
    "    if config[\"mode\"] == \"train\":\n",
    "        val_accuracy = train_model(model, train_dataset, val_dataset, criterion, optimizer, scheduler, config)\n",
    "        return val_accuracy\n",
    "    elif config[\"mode\"] == \"eval\":\n",
    "        test_accuracy = load_and_evaluate_model(model, test_dataset, criterion, optimizer, scheduler, config)\n",
    "        return test_accuracy\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {config['mode']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pretrained model IMAGENET1K_V2\n",
      "Using cuda device\n",
      "Number of model parameters: 23528522 (14985226 trainable)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjunior7ee\u001b[0m (\u001b[33mjunior7ee-university-of-suwon\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/deep_learning_lab_Whatever_you_want/lab_07/wandb/run-20250529_090053-tisqb7x2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/junior7ee-university-of-suwon/CIFAR10-experiments/runs/tisqb7x2' target=\"_blank\">ResNet-50_pretrained</a></strong> to <a href='https://wandb.ai/junior7ee-university-of-suwon/CIFAR10-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/junior7ee-university-of-suwon/CIFAR10-experiments' target=\"_blank\">https://wandb.ai/junior7ee-university-of-suwon/CIFAR10-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/junior7ee-university-of-suwon/CIFAR10-experiments/runs/tisqb7x2' target=\"_blank\">https://wandb.ai/junior7ee-university-of-suwon/CIFAR10-experiments/runs/tisqb7x2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 625/625 [00:23<00:00, 26.98it/s, avg_metrics=Loss: 1.0117e+00 (n=40000), Accuracy:  71.26 (n=40000), Data_Time:  0.002 (n=625), Batch_Time:  0.036 (n=625)]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:04<00:00, 34.49it/s, avg_metrics=Loss: 5.5917e-01 (n=10000), Accuracy:  81.40 (n=10000)]\n",
      "Training Epoch 2: 100%|██████████| 625/625 [00:22<00:00, 28.14it/s, avg_metrics=Loss: 5.0252e-01 (n=40000), Accuracy:  83.24 (n=40000), Data_Time:  0.000 (n=625), Batch_Time:  0.035 (n=625)]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:04<00:00, 34.62it/s, avg_metrics=Loss: 4.5238e-01 (n=10000), Accuracy:  84.68 (n=10000)]\n",
      "Training Epoch 3: 100%|██████████| 625/625 [00:22<00:00, 28.07it/s, avg_metrics=Loss: 3.9223e-01 (n=40000), Accuracy:  86.81 (n=40000), Data_Time:  0.000 (n=625), Batch_Time:  0.035 (n=625)]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:04<00:00, 34.58it/s, avg_metrics=Loss: 4.0564e-01 (n=10000), Accuracy:  86.24 (n=10000)]\n",
      "Training Epoch 4: 100%|██████████| 625/625 [00:22<00:00, 28.10it/s, avg_metrics=Loss: 3.1894e-01 (n=40000), Accuracy:  89.23 (n=40000), Data_Time:  0.000 (n=625), Batch_Time:  0.035 (n=625)]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:04<00:00, 34.69it/s, avg_metrics=Loss: 3.8398e-01 (n=10000), Accuracy:  87.28 (n=10000)]\n",
      "Training Epoch 5: 100%|██████████| 625/625 [00:22<00:00, 28.10it/s, avg_metrics=Loss: 2.5895e-01 (n=40000), Accuracy:  91.56 (n=40000), Data_Time:  0.000 (n=625), Batch_Time:  0.035 (n=625)]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:04<00:00, 34.71it/s, avg_metrics=Loss: 3.6515e-01 (n=10000), Accuracy:  87.81 (n=10000)]\n",
      "Training Epoch 6: 100%|██████████| 625/625 [00:22<00:00, 28.09it/s, avg_metrics=Loss: 2.1093e-01 (n=40000), Accuracy:  93.09 (n=40000), Data_Time:  0.000 (n=625), Batch_Time:  0.035 (n=625)]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:04<00:00, 34.58it/s, avg_metrics=Loss: 3.6569e-01 (n=10000), Accuracy:  88.21 (n=10000)]\n",
      "Training Epoch 7: 100%|██████████| 625/625 [00:22<00:00, 28.12it/s, avg_metrics=Loss: 1.6611e-01 (n=40000), Accuracy:  94.86 (n=40000), Data_Time:  0.000 (n=625), Batch_Time:  0.035 (n=625)]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:04<00:00, 34.62it/s, avg_metrics=Loss: 3.5946e-01 (n=10000), Accuracy:  88.54 (n=10000)]\n",
      "Training Epoch 8: 100%|██████████| 625/625 [00:22<00:00, 28.06it/s, avg_metrics=Loss: 1.3425e-01 (n=40000), Accuracy:  95.85 (n=40000), Data_Time:  0.000 (n=625), Batch_Time:  0.035 (n=625)]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:04<00:00, 34.64it/s, avg_metrics=Loss: 3.6392e-01 (n=10000), Accuracy:  88.64 (n=10000)]\n",
      "Training Epoch 9: 100%|██████████| 625/625 [00:22<00:00, 28.10it/s, avg_metrics=Loss: 1.0416e-01 (n=40000), Accuracy:  96.89 (n=40000), Data_Time:  0.000 (n=625), Batch_Time:  0.035 (n=625)]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:04<00:00, 34.62it/s, avg_metrics=Loss: 3.7191e-01 (n=10000), Accuracy:  88.81 (n=10000)]\n",
      "Training Epoch 10: 100%|██████████| 625/625 [00:22<00:00, 28.04it/s, avg_metrics=Loss: 8.2652e-02 (n=40000), Accuracy:  97.53 (n=40000), Data_Time:  0.001 (n=625), Batch_Time:  0.035 (n=625)]\n",
      "Validation/Test: 100%|██████████| 157/157 [00:04<00:00, 34.76it/s, avg_metrics=Loss: 3.8781e-01 (n=10000), Accuracy:  88.93 (n=10000)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▆▆▇▇███</td></tr><tr><td>Train Loss</td><td>█▄▃▃▂▂▂▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▄▅▆▇▇████</td></tr><tr><td>Validation Loss</td><td>█▄▃▂▁▁▁▁▁▂</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate</td><td>0.01</td></tr><tr><td>Train Accuracy</td><td>97.5275</td></tr><tr><td>Train Loss</td><td>0.08265</td></tr><tr><td>Validation Accuracy</td><td>88.93</td></tr><tr><td>Validation Loss</td><td>0.38781</td></tr><tr><td>epoch</td><td>9</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet-50_pretrained</strong> at: <a href='https://wandb.ai/junior7ee-university-of-suwon/CIFAR10-experiments/runs/tisqb7x2' target=\"_blank\">https://wandb.ai/junior7ee-university-of-suwon/CIFAR10-experiments/runs/tisqb7x2</a><br/> View project at: <a href='https://wandb.ai/junior7ee-university-of-suwon/CIFAR10-experiments' target=\"_blank\">https://wandb.ai/junior7ee-university-of-suwon/CIFAR10-experiments</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250529_090053-tisqb7x2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "88.93"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_train_resnet50(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lab을 마무리 짓기 전 저장된 checkpoint를 모두 지워 저장공간을 확보합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "if os.path.exists('checkpoints/'):\n",
    "    shutil.rmtree('checkpoints/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
